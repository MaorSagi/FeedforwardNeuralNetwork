import random
import warnings
import numpy as np
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import time

'''
Input: 
an array of the dimensions of each layer in the network (layer 0 is 
the size of the flattened input, layer L is the output softmax)

Output: 
a dictionary containing the initialized W and b parameters of 
each layer (W1…WL, b1…bL).
'''


def initialize_parameters(layer_dims):
    layers = []
    for i in range(len(layer_dims) - 1):
        layers.append({})
        layers[i]['W'] = np.random.randn(layer_dims[i + 1], layer_dims[i]) * np.sqrt(2 / layer_dims[i])
        layers[i]['B'] = np.array([np.zeros(layer_dims[i + 1])]).T
    return layers


'''
Description: 
Implement the linear part of a layer's forward propagation.

Input: 
A – the activations of the previous layer
W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
B – the bias vector of the current layer (of shape [size of current layer, 1])

Output:
Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
'''


def linear_forward(A, W, b):
    return (W @ A + b), {'A': A, 'W': W, 'B': b}


'''
Input:
Z – the linear component of the activation function

Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation
'''


def softmax(Z):
    exp_Z = np.exp(Z)
    return exp_Z / exp_Z.sum(0), Z


'''
Input:
Z – the linear component of the activation function

Output:
A – the activations of the layer
activation_cache – returns Z, which will be useful for the backpropagation
'''


def relu(Z):
    A = Z.copy()
    A[A < 0] = 0
    return A, Z


'''
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer

Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “softmax” or “relu”)

Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache

'''


def linear_activation_forward(A_prev, W, B, activation):
    Z, cache_linear = linear_forward(A_prev, W, B)
    A, cache_act = activation(Z)
    cache = cache_linear
    cache['Z'] = cache_act
    return A, cache


'''
Description:
Implement the forward propagation for the LINEAR->ACTIVATION layer when dropout option is on

Input:
A_prev – activations of the previous layer
W – the weights matrix of the current layer
B – the bias vector of the current layer
Activation – the activation function to be used (a string, either “softmax” or “relu”)

Output:
A – the activations of the current layer
cache – a joint dictionary containing both linear_cache and activation_cache

'''


def linear_activation_forward_da(A_prev, W, B, activation):
    Z, cache_linear = linear_forward(A_prev, W, B)
    A, cache_act = activation(Z)
    keep_prob = 0.8
    d_i = np.random.rand(A.shape[0], A.shape[1]) < keep_prob
    A = A * d_i
    A = A / keep_prob
    cache = cache_linear
    cache['Z'] = cache_act
    cache['dropouts_mask'] = d_i
    cache['keep_prob'] = keep_prob
    return A, cache


'''
Description:
Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation

Input:
X – the data, numpy array of shape (input size, number of examples)
parameters – the initialized W and b parameters of each layer
use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).

Output:
AL – the last post-activation value
caches – a list of all the cache objects generated by the linear_forward function
'''


def L_model_forward(X, parameters, use_batchnorm, da=False):
    A = X
    caches = []
    for i in range(len(parameters)):
        net_i = parameters[i]
        if i == (len(parameters) - 1):
            activation = softmax
        else:
            activation = relu
        if da and activation == relu:
            A, cache = linear_activation_forward_da(A, net_i['W'], net_i['B'], activation)
        else:
            A, cache = linear_activation_forward(A, net_i['W'], net_i['B'], activation)
        caches.append(cache)
        if (i != len(parameters) - 1) and use_batchnorm:
            A = apply_batchnorm(A)
    return A, caches


'''
Description:
Implement the cost function defined by equation.
The requested cost function is categorical cross-entropy loss. 

Input:
AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
Y – the labels vector (i.e. the ground truth)

Output:
cost – the cross-entropy cost

'''


def compute_cost(AL, Y):
    m = AL.shape[0]
    logged_samples = np.log(AL + np.finfo(float).eps)
    cost = np.sum(logged_samples * Y)
    cost = -1 * cost# / m
    return cost


'''
Description:
performs batchnorm on the received activation values of a given layer.

Input:
A - the activation values of a given layer

Output:
NA - the normalized activation values, based on the formula learned in class
'''


def apply_batchnorm(A):
    A = (A - A.mean()) / np.sqrt(A.std() ** 2 + np.finfo(float).eps)
    return A


'''
Description:
Implements the linear part of the backward propagation process for a single layer

Input:
dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

Output:
dA_prev - Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
dW - Gradient of the cost with respect to W (current layer l), same shape as W
db - Gradient of the cost with respect to b (current layer l), same shape as b
'''


def linear_backward(dZ, cache):
    dA_prev = cache['W'].T @ dZ
    dW = dZ @ cache['A'].T / dZ.shape[0]  # (dZ.sum(1) @ cache['A'].T) / dZ.shape[0]
    dB = dZ.sum(1) / dZ.shape[0]
    return dA_prev, dW, dB


'''
Description:
Implements the backward propagation for the LINEAR->ACTIVATION layer.
The function first computes dZ and then applies the linear_backward function.

Input:
dA – post activation gradient of the current layer
cache – contains both the linear cache and the activations cache

Output:
dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
dW – Gradient of the cost with respect to W (current layer l), same shape as W
db – Gradient of the cost with respect to b (current layer l), same shape as b
'''


def linear_activation_backward(dA, cache, activation):
    if 'dropouts_mask' in cache:
        dA = dA * cache['dropouts_mask']
        dA = dA / cache['keep_prob']
    if activation == relu:
        dZ = relu_backward(dA, cache)
    else:
        dZ = softmax_backward(dA, cache)
    return linear_backward(dZ, cache)


'''
Description:
Implements backward propagation for a ReLU unit

Input:
dA – the post-activation gradient
activation_cache – contains Z (stored during the forward propagation)

Output:
dZ – gradient of the cost with respect to Z
'''


def relu_backward(dA, activation_cache):
    Z = activation_cache['Z']
    dA[Z <= 0] = 0
    return dA


'''
Description:
Implements backward propagation for a softmax unit

Input:
dA – the post-activation gradient
activation_cache – contains Z (stored during the forward propagation)

Output:
dZ – gradient of the cost with respect to Z
'''


def softmax_backward(dA, activation_cache):
    Y = activation_cache['Y']
    dZ = dA - Y
    return dZ


'''
Description:
Implement the backward propagation process for the entire network.

Input:
AL - the probabilities vector, the output of the forward propagation (L_model_forward)
Y - the true labels vector (the "ground truth" - true classifications)
caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

Output:
  grads - a dictionary with the gradients
'''


def L_model_backward(AL, Y, caches):
    caches[-1]["Y"] = Y
    grads = {}

    dA, dW, dB = linear_activation_backward(AL, caches[-1], softmax)
    grads['dA' + str(len(caches) - 1)] = dA
    grads['dW' + str(len(caches) - 1)] = dW
    grads['dB' + str(len(caches) - 1)] = dB
    for i, cache in zip(range(len(caches) - 2, -1, -1), reversed(caches[:-1])):
        dA, dW, dB = linear_activation_backward(dA, cache, relu)
        grads['dA' + str(i)] = dA
        grads['dW' + str(i)] = dW
        grads['dB' + str(i)] = dB

    return grads


'''
Description:
Updates parameters using gradient descent

Input:
parameters – a python dictionary containing the DNN architecture’s parameters
grads – a python dictionary containing the gradients (generated by L_model_backward)
learning_rate – the learning rate used to update the parameters (the “alpha”)

Output:
parameters – the updated values of the parameters object provided as input
'''


def update_parameters(parameters, grads, learning_rate):
    for i, layer in enumerate(parameters):
        layer['W'] = layer['W'] - learning_rate * grads['dW' + str(i)]
        layer['B'] = (layer['B'].T - learning_rate * grads['dB' + str(i)]).T
    return parameters


'''
Description:
Implements a L-layer neural network. All layers but the last should have the ReLU activation function, 
and the final layer will apply the softmax activation function. The size of the output layer should be
equal to the number of labels in the data. Please select a batch size that enables your code to 
run well (i.e. no memory overflows while still running relatively fast).

Input:
X – the input data, a numpy array of shape (height*width , number_of_examples) 
Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
layer_dims – a list containing the dimensions of each layer, including the input
batch_size – the number of examples in a single training batch.

Output:
parameters – the parameters learnt by the system during the training (the same parameters that were updated in the update_parameters function).
costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).
'''


def L_layer_model(x_train, y_train, layers_dims, learning_rate, num_iterations, batch_size):
    indcs_val = np.random.choice(x_train.shape[0], 12000, replace=False)

    indcs_train = [i for i in range(60000) if i not in indcs_val]
    x_val = x_train[indcs_val]
    x_train = x_train[indcs_train]
    y_val = y_train[indcs_val]
    y_train = y_train[indcs_train]

    print("x_train shape:", x_train.shape)
    print(x_train.shape[0], "train samples")
    print(x_test.shape[0], "test samples")
    print(x_val.shape[0], "test samples")
    use_batchnorm = False
    use_dropouts = False
    net = initialize_parameters(layers_dims)
    costs = {'train': [], 'validation': []}
    acc_cost = 0
    last_cost = 99999999
    for epoch in range(num_iterations):
        print(epoch)
        X_splatted = np.split(x_train, int(x_train.shape[0] / batch_size))
        Y_splatted = np.split(y_train, int(x_train.shape[0] / batch_size))
        for X_batch, Y_batch in zip(X_splatted, Y_splatted):
            X_batch = X_batch.T
            Y_batch = Y_batch.T
            A, caches = L_model_forward(X_batch, net, use_batchnorm, use_dropouts)
            grads = L_model_backward(A, Y_batch, caches)
            update_parameters(net, grads, learning_rate)
        if epoch % 100 == 0:
            for X_batch, Y_batch in zip(X_splatted, Y_splatted):
                X_batch = X_batch.T
                Y_batch = Y_batch.T
                A, caches = L_model_forward(X_batch, net, use_batchnorm)
                acc_cost += compute_cost(A, Y_batch)
            acc_cost = acc_cost / x_train.shape[0]
            costs['train'].append(acc_cost)

            if x_val is not None:
                X_val_splatted = np.split(x_val, int(x_val.shape[0] / 10))
                Y_val_splatted = np.split(y_val, int(x_val.shape[0] / 10))
                acc_val_cost = 0
                for X_batch, Y_batch in zip(X_val_splatted, Y_val_splatted):
                    X_batch = X_batch.T
                    Y_batch = Y_batch.T
                    A, caches = L_model_forward(X_batch, net, use_batchnorm)
                    acc_val_cost += compute_cost(A, Y_batch)
                acc_val_cost = acc_val_cost / x_val.shape[0]
                costs['validation'].append(acc_val_cost)
                if acc_val_cost > last_cost * 1.05:
                    with open("results " + ("BATCHNORM=ON" if use_batchnorm else "BATCHNORM=OFF") + ".txt", "a") as f:
                        f.write('epochs: ' + str(epoch) +'. no improvement - breaking\n')
                        f.write('acc_val_cost > last_cost : '+ str(acc_val_cost)+ '>'+ str(last_cost) +'\n')
                    parameters = {"net": net, "use_batchnorm": use_batchnorm}
                    val_prediction = Predict(x_val, y_val, parameters)
                    train_prediction = Predict(x_train, y_train, parameters)
                    parameters["train_prediction"] = train_prediction
                    parameters["val_prediction"] = val_prediction
                    return parameters, costs
                last_cost = acc_val_cost
            acc_cost = 0
    parameters = {"net": net, "use_batchnorm": use_batchnorm}
    val_prediction = Predict(x_val, y_val, parameters)
    train_prediction = Predict(x_train, y_train, parameters)
    parameters["train_prediction"] = train_prediction
    parameters["val_prediction"] = val_prediction
    return parameters, costs


'''
Description:
The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.

Input:
X – the input data, a numpy array of shape (height*width, number_of_examples)
Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
parameters – a python dictionary containing the DNN architecture’s parameters

Output:
accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for which the correct label receives the hughest confidence score). Use the softmax function to normalize the output values.
'''


def Predict(X, Y, parameters):
    net = parameters['net']
    use_batchnorm = parameters['use_batchnorm']
    output, _ = L_model_forward(X.T, net, use_batchnorm)
    prediction = output.argmax(0)
    Y = Y.argmax(1)
    correct = (prediction == Y).sum()
    return correct / X.shape[0]


'''
Classify the MNIST dataset
'''
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255
num_classes = 10
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
x_train = x_train.reshape([x_train.shape[0], 28 * 28])
x_test = x_test.reshape([x_test.shape[0], 28 * 28])

print("x_train shape:", x_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")

layer_dims = [784, 20, 7, 5, 10]
learning_rate = 0.009
batch_sizes = [50,100,500]
for batch_size in batch_sizes:
    start = time.time()
    parameters, costs = L_layer_model(x_train, y_train, layer_dims, learning_rate, 1000, batch_size)
    test_prediction = Predict(x_test, y_test, parameters)
    end = time.time()
    with open("results " + ("BATCHNORM=ON" if parameters['use_batchnorm'] else "BATCHNORM=OFF") + ".txt", "a") as f:
        f.write("BATCH_SIZE=" + str(batch_size) + ("\n"))
        f.write("Costs:\n" + str(costs) + ("\n"))
        f.write("Train Prediction: " + str(parameters['train_prediction']) + ("\n"))
        f.write("Validation Prediction: " + str(parameters['val_prediction']) + ("\n"))
        f.write("Test Prediction: " + str(test_prediction) + ("\n"))
        f.write("Time: " + str(end - start) + " sec\n")
